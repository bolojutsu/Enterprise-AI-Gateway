use crate::gateway::llm_service_server::LlmService;
use crate::gateway::{PromptRequest, PromptResponse};
use crate::state::AppState;

use std::sync::{atomic::Ordering, Arc};
use tokio::select;
use tokio::time::{timeout, Duration};
use tonic::{Request, Response, Status};
pub struct GatewayService {
    pub state: Arc<AppState>,
}

#[tonic::async_trait]
impl LlmService for GatewayService {
    async fn execute_prompt(
        &self,
        request: Request<PromptRequest>,
    ) -> Result<Response<PromptResponse>, Status> {
        // Increment global state
        self.state.total_requests.fetch_add(1, Ordering::Relaxed);

        let req = request.into_inner();
        println!("ðŸ“ Request received. User Prompt: '{}'", req.user_prompt);
        let final_text = self.model_race(&req.user_prompt).await;

        Ok(Response::new(PromptResponse {
            text: final_text,
            cost: 0.015,
        }))
    }
}

impl GatewayService {
    /// The "Race Pattern": Fires multiple models simultaneously.
    /// The first one to finish wins. The loser is automatically cancelled.
    async fn model_race(&self, prompt: &str) -> String {
        println!("ðŸŽï¸  Starting the Model Race...");

        // Create the "Futures" (the tasks), but don't .await them yet!
        let primary_call = self.mock_api_call("GPT-4 (Expensive/Slow)", prompt, 3000);
        let fallback_call = self.mock_api_call("Llama-3 (Fast/Local)", prompt, 500);

        // Pin the futures so select! can track them in memory
        tokio::pin!(primary_call);
        tokio::pin!(fallback_call);

        select! {
            res = primary_call => {
                println!("ðŸ¥‡ GPT-4 won the race!");
                res
            }
            res = fallback_call => {
                println!("ðŸ¥‡ Llama-3 won the race!");
                res
            }
        }
    }

    // Simulates a call with a specific delay
    async fn mock_api_call(&self, model: &str, prompt: &str, delay_ms: u64) -> String {
        tokio::time::sleep(Duration::from_millis(delay_ms)).await;
        format!("Generated by {}: Response for '{}'", model, prompt)
    }

    // Handels the primary-secondary logic
    // if the primary model takes longer that 2 seconds, we pivot to the fallback.
    // async fn call_with_failover(&self, prompt: &str) -> String {
    //     println!("ðŸ•’ Attempting Primary Model...");
    //     match timeout(Duration::from_secs(2), self.mock_llm_call("GPT-4", prompt)).await {
    //         Ok(response) => {
    //             println!("âœ… Primary Model succeeded.");
    //             response
    //         }
    //         Err(_) => {
    //             println!("âš ï¸ Primary timed out! Switching to Fallback...");
    //             self.mock_llm_call("Llama-3 (Local)", prompt).await
    //         }
    //     }
    // }

    // helper function to simulate a network call to an AI provider,
    // in a real app this would use reqwest to hit OpenAi or Ollama
    // async fn mock_llm_call(&self, model_name: &str, prompt: &str) -> String {
    //     let delay: u64;
    //     if model_name == "GPT-4" {
    //         delay = 3000;
    //     } else {
    //         delay = 500;
    //     }
    //     tokio::time::sleep(Duration::from_millis(delay)).await;
    //     format!("Response from {}: '{}'", model_name, prompt)
    // }
}
