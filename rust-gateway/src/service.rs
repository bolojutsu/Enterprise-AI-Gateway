use crate::clients::tavily::call_tavily;
use crate::gateway::llm_service_server::LlmService;
use crate::gateway::{PromptRequest, PromptResponse};
use crate::state::AppState;

use crate::clients::claud::call_claude;
use crate::clients::gemini::call_gemini;
use crate::clients::grok::call_grok;
use crate::clients::openai::call_openai;

use std::sync::{atomic::Ordering, Arc};
use tokio::select;

use tonic::{Request, Response, Status};
pub struct GatewayService {
    pub state: Arc<AppState>,
}

#[tonic::async_trait]
impl LlmService for GatewayService {
    async fn execute_prompt(
        &self,
        request: Request<PromptRequest>,
    ) -> Result<Response<PromptResponse>, Status> {
        // Increment global state
        self.state.total_requests.fetch_add(1, Ordering::Relaxed);
        let req = request.into_inner();
        println!("ðŸ“ Request received. User Prompt: '{}'", req.user_prompt);

        let final_text: String;
        if req.model == "research" {
            final_text = self.research_and_summarize(&req.user_prompt).await
        } else {
            final_text = self.model_race(&req.user_prompt).await
        }

        Ok(Response::new(PromptResponse {
            text: final_text,
            cost: 0.015,
        }))
    }
}

impl GatewayService {
    /// The "Race Pattern": Fires multiple models simultaneously.
    /// The first one to finish wins. The loser is automatically cancelled.
    async fn model_race(&self, prompt: &str) -> String {
        println!("ðŸŽï¸  Starting the Real-World Model Race...");

        let openai_task = call_openai(prompt, "gpt-4o");
        let grok_task = call_grok(prompt, "grok-beta");
        let gemini_task = call_gemini(prompt, "gemini-1.5-flask");
        let claude_task = call_claude(prompt, "claude-4o");

        tokio::pin!(openai_task);
        tokio::pin!(grok_task);
        tokio::pin!(gemini_task);

        select! {
            res = openai_task => self.format_winner("OpenAI", res),
            res = grok_task => self.format_winner("Grok", res),
            res = gemini_task => self.format_winner("Gemini", res),
            res = claude_task => self.format_winner("Claude", res),
        }
    }

    // helper method to format the winner
    fn format_winner(
        &self,
        name: &str,
        result: Result<String, crate::clients::ClientError>,
    ) -> String {
        match result {
            Ok(text) => {
                println!("{} won the race!", name);
                text
            }
            Err(e) => {
                println!("âŒ {} failed: {:?}", name, e);
                format!("Error from {}: {:?}", name, e)
            }
        }
    }

    async fn research_and_summarize(&self, prompt: &str) -> String {
        println!("Reasurching via Tavily");

        // 1. get search results
        let search_results = match call_tavily(prompt).await {
            Ok(results) => results,
            Err(e) => return format!("Search failed: {:?}", e),
        };

        // feed results to a fast model like gemini for summary
        let grounded_prompt = format!(
            "Based on these search results: \n\n{}\n\n Please answer the user: {}",
            search_results, prompt
        );

        match call_gemini(&grounded_prompt, "gemini-1.5-flash").await {
            Ok(summary) => summary,
            Err(_) => search_results,
        }
    }

    // Simulates a call with a specific delay
    // async fn mock_api_call(&self, model: &str, prompt: &str, delay_ms: u64) -> String {
    //     tokio::time::sleep(Duration::from_millis(delay_ms)).await;
    //     format!("Generated by {}: Response for '{}'", model, prompt)
    // }

    // Handels the primary-secondary logic
    // if the primary model takes longer that 2 seconds, we pivot to the fallback.
    // async fn call_with_failover(&self, prompt: &str) -> String {
    //     println!("ðŸ•’ Attempting Primary Model...");
    //     match timeout(Duration::from_secs(2), self.mock_llm_call("GPT-4", prompt)).await {
    //         Ok(response) => {
    //             println!("âœ… Primary Model succeeded.");
    //             response
    //         }
    //         Err(_) => {
    //             println!("âš ï¸ Primary timed out! Switching to Fallback...");
    //             self.mock_llm_call("Llama-3 (Local)", prompt).await
    //         }
    //     }
    // }

    // helper function to simulate a network call to an AI provider,
    // in a real app this would use reqwest to hit OpenAi or Ollama
    // async fn mock_llm_call(&self, model_name: &str, prompt: &str) -> String {
    //     let delay: u64;
    //     if model_name == "GPT-4" {
    //         delay = 3000;
    //     } else {
    //         delay = 500;
    //     }
    //     tokio::time::sleep(Duration::from_millis(delay)).await;
    //     format!("Response from {}: '{}'", model_name, prompt)
    // }
}
