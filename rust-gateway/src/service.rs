use crate::gateway::llm_service_server::LlmService;
use crate::gateway::{PromptRequest, PromptResponse};
use crate::state::AppState;

use crate::clients::claud::call_claude;
use crate::clients::gemini::call_gemini;
use crate::clients::grok::call_grok;
use crate::clients::openai::call_openai;

use std::sync::{atomic::Ordering, Arc};
use tokio::select;

use tonic::{Request, Response, Status};
pub struct GatewayService {
    pub state: Arc<AppState>,
}

#[tonic::async_trait]
impl LlmService for GatewayService {
    async fn execute_prompt(
        &self,
        request: Request<PromptRequest>,
    ) -> Result<Response<PromptResponse>, Status> {
        // Increment global state
        self.state.total_requests.fetch_add(1, Ordering::Relaxed);

        let req = request.into_inner();
        println!("ðŸ“ Request received. User Prompt: '{}'", req.user_prompt);
        let final_text = self.model_race(&req.user_prompt).await;

        Ok(Response::new(PromptResponse {
            text: final_text,
            cost: 0.015,
        }))
    }
}

impl GatewayService {
    /// The "Race Pattern": Fires multiple models simultaneously.
    /// The first one to finish wins. The loser is automatically cancelled.
    async fn model_race(&self, prompt: &str) -> String {
        println!("ðŸŽï¸  Starting the Model Race...");

        // Create the "Futures" (the tasks), but don't .await them yet!
        let task1 = call_openai(prompt, "gpt-4o");
        let task2 = call_gemini(prompt, "gemini-1.5-flash");
        let task3 = call_claude(prompt, "claude-3-haiku");
        let task4 = call_grok(prompt, "grok-4");

        // Pin the futures so select! can track them in memory
        tokio::pin!(task1);
        tokio::pin!(task2);
        tokio::pin!(task3);
        tokio::pin!(task4);

        select! {
            res = task1 => format!("ðŸ¥‡ OpenAI won: {}", res.unwrap_or_default()),
            res = task2 => format!("ðŸ¥‡ Gemini won: {}", res.unwrap_or_default()),
            res = task3 => format!("ðŸ¥‡ Claude won: {}", res.unwrap_or_default()),
            res = task4 => format!("ðŸ¥‡ Grok Won: {}", res.unwrap_or_default()),
        }
    }

    // Simulates a call with a specific delay
    // async fn mock_api_call(&self, model: &str, prompt: &str, delay_ms: u64) -> String {
    //     tokio::time::sleep(Duration::from_millis(delay_ms)).await;
    //     format!("Generated by {}: Response for '{}'", model, prompt)
    // }

    // Handels the primary-secondary logic
    // if the primary model takes longer that 2 seconds, we pivot to the fallback.
    // async fn call_with_failover(&self, prompt: &str) -> String {
    //     println!("ðŸ•’ Attempting Primary Model...");
    //     match timeout(Duration::from_secs(2), self.mock_llm_call("GPT-4", prompt)).await {
    //         Ok(response) => {
    //             println!("âœ… Primary Model succeeded.");
    //             response
    //         }
    //         Err(_) => {
    //             println!("âš ï¸ Primary timed out! Switching to Fallback...");
    //             self.mock_llm_call("Llama-3 (Local)", prompt).await
    //         }
    //     }
    // }

    // helper function to simulate a network call to an AI provider,
    // in a real app this would use reqwest to hit OpenAi or Ollama
    // async fn mock_llm_call(&self, model_name: &str, prompt: &str) -> String {
    //     let delay: u64;
    //     if model_name == "GPT-4" {
    //         delay = 3000;
    //     } else {
    //         delay = 500;
    //     }
    //     tokio::time::sleep(Duration::from_millis(delay)).await;
    //     format!("Response from {}: '{}'", model_name, prompt)
    // }
}
